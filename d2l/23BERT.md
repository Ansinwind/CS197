# BERT



**NLP的迁移学习**

- 使用预训练好的模型来抽取词、句子的特征
  - 例如word2vec或语言模型
- 不更新预训练好的模型
- 需要构建新的网络来抓取新任务需要的信息
  - word2vec忽略了时序信息，语言模型只看了一个方向



**动机**

- 基于微调的NLP模型
- 预训练的模型抽取了足够多的信息
- 新的任务只需要增加一个简单的输出层



## BERT架构

- 只有编码器的Transformer
- 对输入的修改
  - 每个样本是一个句子对
  - 加入额外的片段嵌入
  - 位置编码可学习
- 针对微调设计修改
  - 模型更大，训练数据更多
  - 输入句子对，片段嵌入，可学习的位置编码
  - 训练时使用两个任务（没啥用）：
    - 带掩码的语言模型
    - 下一个句子预测



## BERT微调

- BERT对每一个词元返回抽取了上下文信息的特征向量
- 不同的任务使用不同的特性



**句子分类**

将<cls>对应的向量输入到全连接层分类

**命名实体识别**

将非特殊次元放进全连接层分类

**问题回答**

- 给定一个问题和描述文字，找出一个片段作为回答
- 对片段中的每个词元预测他是不是回答的开头或结束



